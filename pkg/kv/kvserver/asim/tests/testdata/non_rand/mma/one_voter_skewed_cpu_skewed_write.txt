# This test verifies the allocator's behavior with replication factor 1 and
# skewed workloads across two stores. The test sets up a 2-node cluster where
# store s1 handles all read traffic (high CPU load from request processing)
# while store s2 handles all write traffic (higher write bandwidth).
#
# Expected outcome: two stores should roughly equalize their cpu load and write
# load via range rebalancing.
gen_cluster nodes=2 node_cpu_rate_capacity=1000000000
----

gen_ranges ranges=100 repl_factor=1 min_key=1 max_key=10000 placement_type=replica_placement bytes=26843545
{s1}:1
----
{s1:*}:1

gen_ranges ranges=100 repl_factor=1 min_key=10001 max_key=20000 placement_type=replica_placement bytes=26843545
{s2}:1
----
{s2:*}:1

# read cpu load of 1000x100=10k, all hitting s1, which is then at 100% cpu.
gen_load rate=1000 rw_ratio=1.0 request_cpu_per_access=500000 min_key=1 max_key=10000
----

# Write only workload, which generates 20% cpu and 5mb of writes per second.
# over the second half of the keyspace.
gen_load rate=5000 rw_ratio=0 min_block=1000 max_block=1000 raft_cpu_per_write=1 min_key=10001 max_key=20000
----

setting split_queue_enabled=false
----

eval duration=65m samples=1 seed=42 cfgs=(mma-only,mma-count) metrics=(cpu,cpu_util,write_bytes_per_second,replicas,leases)
----
cpu#1: last:  [s1=269737582, s2=229946569] (stddev=19895506.50, mean=249842075.50, sum=499684151)
cpu#1: thrash_pct: [s1=6%, s2=52%]  (sum=58%)
cpu_util#1: last:  [s1=0.27, s2=0.23] (stddev=0.02, mean=0.25, sum=0)
cpu_util#1: thrash_pct: [s1=6%, s2=52%]  (sum=58%)
leases#1: first: [s1=100, s2=100] (stddev=0.00, mean=100.00, sum=200)
leases#1: last:  [s1=99, s2=101] (stddev=1.00, mean=100.00, sum=200)
leases#1: thrash_pct: [s1=1470%, s2=1470%]  (sum=2940%)
replicas#1: first: [s1=100, s2=100] (stddev=0.00, mean=100.00, sum=200)
replicas#1: last:  [s1=99, s2=101] (stddev=1.00, mean=100.00, sum=200)
replicas#1: thrash_pct: [s1=1470%, s2=1470%]  (sum=2940%)
write_bytes_per_second#1: last:  [s1=2249075, s2=2750166] (stddev=250545.50, mean=2499620.50, sum=4999241)
write_bytes_per_second#1: thrash_pct: [s1=24%, s2=3%]  (sum=27%)
artifacts[mma-only]: 5d019c9eec51a880
==========================
cpu#1: last:  [s1=269737582, s2=229946569] (stddev=19895506.50, mean=249842075.50, sum=499684151)
cpu#1: thrash_pct: [s1=6%, s2=52%]  (sum=58%)
cpu_util#1: last:  [s1=0.27, s2=0.23] (stddev=0.02, mean=0.25, sum=0)
cpu_util#1: thrash_pct: [s1=6%, s2=52%]  (sum=58%)
leases#1: first: [s1=100, s2=100] (stddev=0.00, mean=100.00, sum=200)
leases#1: last:  [s1=99, s2=101] (stddev=1.00, mean=100.00, sum=200)
leases#1: thrash_pct: [s1=1470%, s2=1470%]  (sum=2940%)
replicas#1: first: [s1=100, s2=100] (stddev=0.00, mean=100.00, sum=200)
replicas#1: last:  [s1=99, s2=101] (stddev=1.00, mean=100.00, sum=200)
replicas#1: thrash_pct: [s1=1470%, s2=1470%]  (sum=2940%)
write_bytes_per_second#1: last:  [s1=2249195, s2=2750240] (stddev=250522.50, mean=2499717.50, sum=4999435)
write_bytes_per_second#1: thrash_pct: [s1=24%, s2=3%]  (sum=27%)
artifacts[mma-count]: 2d5d07f53dca7040
==========================
