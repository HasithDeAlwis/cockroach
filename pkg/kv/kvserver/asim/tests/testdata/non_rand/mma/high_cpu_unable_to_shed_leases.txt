# This test verifies that for remotely cpu-overloaded stores, mma wait for lease
# shedding grace period (remoteStoreLeaseSheddingGraceDuration) before rebalancing
# replicas away from the store. The test sets up a 5-node cluster where store s1
# has a high replica count (25 out of 75 total replicas) but holds no leases. All
# leases are distributed among stores s2-s5. A write-only workload with high raft
# CPU cost creates CPU pressure primarily on s1 due to its replica count, but s1
# cannot shed leases since it holds none.
#
# Expected outcome:
# Want to test two cases:
# (1) high_cpu_unable_to_shed_leases.txt: Where its impossible to shed leases
# from the cpu overloaded s1, so we should initially observe a period of no
# rebalancing activity away from the store before
# any replica based rebalancing.
# (2) high_cpu_able_to_shed_leases.txt: Where its possible to shed leases from
# the CPU overloaded s1, so we should observe a period of lease transfers before
# any replica based rebalancing away from the store occurs.

gen_cluster nodes=5 node_cpu_rate_capacity=9000000000
----

setting split_queue_enabled=false
----

gen_ranges ranges=25 min_key=0 max_key=10000 placement_type=replica_placement
{s1,s2:*,s3}:7
{s1,s4,s5:*}:6
{s1,s2,s4:*}:6
{s1,s3:*,s5}:6
----
{s1,s2:*,s3}:7
{s1,s4,s5:*}:6
{s1,s2,s4:*}:6
{s1,s3:*,s5}:6

gen_load rate=5000 rw_ratio=0 min_key=0 max_key=10000 raft_cpu_per_write=1000000
----

eval duration=30m samples=1 seed=42 cfgs=(mma-only,mma-count) metrics=(cpu,cpu_util,write_bytes_per_second,replicas,leases)
----
cpu#1: last:  [s1=2600894000, s2=3001220000, s3=3199993333, s4=3002060000, s5=3200832666] (stddev=218914332.20, mean=3000999999.80, sum=15004999999)
cpu#1: thrash_pct: [s1=6%, s2=11%, s3=11%, s4=12%, s5=11%]  (sum=51%)
cpu_util#1: last:  [s1=0.29, s2=0.33, s3=0.36, s4=0.33, s5=0.36] (stddev=0.02, mean=0.33, sum=2)
cpu_util#1: thrash_pct: [s1=6%, s2=11%, s3=11%, s4=12%, s5=11%]  (sum=51%)
leases#1: first: [s1=0, s2=7, s3=6, s4=6, s5=6] (stddev=2.53, mean=5.00, sum=25)
leases#1: last:  [s1=0, s2=7, s3=6, s4=6, s5=6] (stddev=2.53, mean=5.00, sum=25)
leases#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%]  (sum=0%)
replicas#1: first: [s1=25, s2=13, s3=13, s4=12, s5=12] (stddev=5.02, mean=15.00, sum=75)
replicas#1: last:  [s1=13, s2=15, s3=16, s4=15, s5=16] (stddev=1.10, mean=15.00, sum=75)
replicas#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%]  (sum=0%)
write_bytes_per_second#1: last:  [s1=2600, s2=3001, s3=3199, s4=3002, s5=3200] (stddev=218.91, mean=3000.40, sum=15002)
write_bytes_per_second#1: thrash_pct: [s1=14%, s2=10%, s3=11%, s4=13%, s5=12%]  (sum=60%)
artifacts[mma-only]: 3334492f2bf1a359
==========================
cpu#1: last:  [s1=2600982666, s2=3202024666, s3=3201089333, s4=3000513333, s5=3000390000] (stddev=219299399.89, mean=3000999999.60, sum=15004999998)
cpu#1: thrash_pct: [s1=24%, s2=23%, s3=22%, s4=19%, s5=20%]  (sum=108%)
cpu_util#1: last:  [s1=0.29, s2=0.36, s3=0.36, s4=0.33, s5=0.33] (stddev=0.02, mean=0.33, sum=2)
cpu_util#1: thrash_pct: [s1=24%, s2=23%, s3=22%, s4=19%, s5=20%]  (sum=108%)
leases#1: first: [s1=0, s2=7, s3=6, s4=6, s5=6] (stddev=2.53, mean=5.00, sum=25)
leases#1: last:  [s1=0, s2=7, s3=6, s4=6, s5=6] (stddev=2.53, mean=5.00, sum=25)
leases#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%]  (sum=0%)
replicas#1: first: [s1=25, s2=13, s3=13, s4=12, s5=12] (stddev=5.02, mean=15.00, sum=75)
replicas#1: last:  [s1=13, s2=16, s3=16, s4=15, s5=15] (stddev=1.10, mean=15.00, sum=75)
replicas#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%]  (sum=0%)
write_bytes_per_second#1: last:  [s1=2600, s2=3202, s3=3201, s4=3000, s5=3000] (stddev=219.64, mean=3000.60, sum=15003)
write_bytes_per_second#1: thrash_pct: [s1=19%, s2=22%, s3=18%, s4=16%, s5=20%]  (sum=96%)
artifacts[mma-count]: 5b1fca7fda20dfdf
==========================
